{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKxJwrGY7mDjQNYwGwr9Q/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasib0077/nlp1/blob/main/pipeline_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mRE6iqSVJT0h",
        "outputId": "ecf5dbda-d5ae-4331-8233-75c7c58254f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs=[\n",
        "     \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\"\n",
        "]\n",
        "inputs=tokenizer(raw_inputs, padding=True, truncation=True,return_tensors=\"tf\")\n"
      ],
      "metadata": {
        "id": "YcdAmIf-LEav"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "checkpoint=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model=TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4_jiTG8NK1g",
        "outputId": "1264c5ba-12e7-49f5-ae9b-df0f8c4869a1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=model(inputs)\n",
        "outputs.logits.shape #two sentences two lebeled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgeEOpvoW5RG",
        "outputId": "7f8756f7-c5e1-4edd-c2d9-3feee9bef628"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.logits #Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEqi9478W_8r",
        "outputId": "56836359-3eb9-474a-d9d7-95f825b21060"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[-1.5606961,  1.6122813],\n",
              "       [ 4.1692314, -3.3464477]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one\n",
        "import tensorflow as tf\n",
        "predictions=tf.math.softmax(outputs.logits,axis=-1)\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY_-oLU1XjhM",
        "outputId": "cf7b0c23-3dad-4c29-f12c-8ee1499a7bed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[4.0195391e-02, 9.5980465e-01],\n",
              "       [9.9945587e-01, 5.4418371e-04]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To get the labels corresponding to each position, we can inspect the id2label attribute of the model config\n",
        "model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsBHZOY7ZH6O",
        "outputId": "d342209f-efc7-4523-8aef-b0a9f8faaeee"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}